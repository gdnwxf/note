rocketmq 源码分析

-------------------------------------------------
	http://hawtio-prome-prod.cloudapps.2dfire-pre.com/hawtio/jmx/attributes?main-tab=jmx#%2Fjvm%2Fconnect%3Fmain-tab=jvm
	https://blog.csdn.net/iie_libi/article/details/54236502
	https://blog.csdn.net/chunlongyu/article/category/6638499

	namesvr 详解
		https://blog.csdn.net/mr253727942/article/details/52637126
	为什么RocketMQ要去除ZK依赖？
		https://blog.csdn.net/chunlongyu/article/details/54018010
	阿里中间件团队
		http://jm.taobao.org/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/
	Broker与NameServer心跳机制
		https://blog.csdn.net/chunlongyu/article/details/54380626
	Consumer负载均衡与Kafka的Consumer负载均衡之不同点
		https://blog.csdn.net/chunlongyu/article/details/54630651
	CommitLog与ConsumeQueue
		https://blog.csdn.net/chunlongyu/article/details/54576649
	Consumer负载均衡机制 -- Rebalance
		https://blog.csdn.net/chunlongyu/article/details/54585232
	Topic路由数据结构解析 -- topicRoute与topicPublishInfo与queueId
		https://blog.csdn.net/chunlongyu/article/details/54376920
	//roketmq 查询命令
		https://www.cnblogs.com/gmq-sh/p/6232633.html  
	//rocketmq原理：name server ，broker， producer， consumer之间通信
		https://blog.csdn.net/iie_libi/article/details/54236502
	//mq 总体简介
		http://www.tianshouzhi.com/api/tutorials/rocketmq

	pull 模式 和 push模式
		pull 拉消息策略  pull -> 本地ProcessQueue  TreeMap<Long, MessageExt> msgTreeMap = new TreeMap<Long, MessageExt>();
									-> 消息处理 解耦
			** MQPullConsumerScheduleService 内部比我们实现了负载均衡
			** DefaultMQPullConsumer 则需要自己实现负载均衡
			Pull的负载均衡
			在MQPullConsumer这个类里面，有一个MessageQueueListener，它的目的就是当queue发生变化的时候，通知Consumer。也正是这个借口，帮助我们在Pull模式里面，实现负载均衡。
			// reblance 由 
			有了这个Listener，我们就可以动态的知道当前的Consumer分摊到了几个MessageQueue。然后对这些MessageQueue，我们可以开个线程池来消费。
		push
			DefaultMQPushConsumer 推送模式
	namesvr 的作用
		NameServer维护了一份Broker的地址列表和，broker在启动的时候会去NameServer进行注册，会维护Broker的存活状态.
		NameServer维护了一份Topic和Topic对应队列的地址列表,broker每次发送心跳过来的时候都会把Topic信息带上
	NameSrv监测Broker的死亡
		机制之一：监测连接断掉
		机制之二：心跳
			每个Broker会每隔30s向NameSrv更新自身topic信息
			NameServer收到RegisterBroker信息，更新自己的brokerLiveTable结构
			然后NameServer会每10s，扫描一次这个结构。如果发现上次更新时间距离当前时间超过了BROKER_CHANNEL_EXPIRED_TIME = 1000 * 60 * 2（2分钟)，则认为此broker死亡。
	Producer/Consumer如何得知Broker死亡
		当某个Broker死亡之后，NameSrv并不会主动通知Producer和Consumer。
		而是Producer/Consumer周期性的去NameSrv取。
	reblance 负载均衡  // https://blog.csdn.net/chunlongyu/article/details/54630651  客户端每隔20s做一次更新
		要做负载均衡，首先要解决的一个问题就是收集信息。所谓收集信息，就是我得知道每一个consumer group都有哪些consumer，对应的topic是谁？
		这样一份全局的信息，
		是存放在Broker，
		还是NameServer上面呢？
		RocketMQ选择了存放在Broker上面。具体做法是：客户端会通过心跳消息，不停的上报自己，RegisterConsumer。 //  放到了ConsumerManager currentHashMap< String/* Group */,ConsumerGroupInfo>
	消息清理
		 扫描间隔
		     默认10秒，由broker配置参数cleanResourceInterval决定
		 空间阈值
		     物理文件不能无限制的一直存储在磁盘，当磁盘空间达到阈值时，不再接受消息，broker打印出日志，消息发送失败，阈值为固定值85%
		 清理时机
		     默认每天凌晨4点，由broker配置参数deleteWhen决定；或者磁盘空间达到阈值
		 文件保留时长
		     默认72小时，由broker配置参数fileReservedTime决定
	可靠性
	 	所有发往broker的消息，有同步刷盘和异步刷盘机制，总的来说，可靠性非常高
	 	同步刷盘时，消息写入物理文件才会返回成功，因此非常可靠
	 	异步刷盘时，只有机器宕机，才会产生消息丢失，broker挂掉可能会发生，但是机器宕机崩溃是很少发生的，除非突然断电
 	读写性能
		文件内存映射方式操作文件，避免read/write系统调用和实时文件读写，性能非常高
		永远一个文件在写，其他文件在读
		顺序写，随机读
		利用linux的sendfile？？？mmap+write吧机制，将消息内容直接输出到sokect管道，避免系统调用
	系统特性
	 	大内存，内存越大性能越高，否则系统swap会成为性能瓶颈
	 	IO密集
	 	cpu load高，使用率低，因为cpu占用后，大部分时间在IO WAIT
	 	磁盘可靠性要求高，为了兼顾安全和性能，采用RAID10阵列
	 	磁盘读取速度要求快，要求高转速大容量磁盘


	 	namesrv 
	 		broker启动，将自身创建的topic等信息注册到Namesrv上, consumer和producer需要配置namesrv的地址，启动后，首先和namesrv建立长连接，并获取相应的topic信息(比如，哪些broker有topic路由信息)，然后再和broker建立长连接。
	 		NamesrvStartUp:NameServer启动类
			NamesrvController:NameServer控制类，管控NameServer的启动、初始化、停止等生命周期
			RouteInfoManager:这个类非常重要，存放了topic队列信息，broker地址列表等一系列重要数据结构，并提供了对应的数据变更接口。
			DefaultRequestProcessosr:负责处理所broker发过来的所有网络消息，封装了对netty包的处理和部分对nameServer存储的数据查询和删除。
			----------------------------------------------------------------------------------------
	 		private final HashMap<String/* topic */, List<QueueData>> topicQueueTable;//topic队列表，存储了每个topic包含的队列数据
			private final HashMap<String/* brokerName */, BrokerData> brokerAddrTable; //broker地址表						
			private final HashMap<String/* clusterName */, Set<String/* brokerName */>> clusterAddrTable; //集群主备信息表
			//其中的BrokerLiveInfo存储了broker的版本号，channel，和最近心跳时间等信息
			private final HashMap<String/* brokerAddr */, BrokerLiveInfo> brokerLiveTable; //broker存活状态信息表，
			//记录了每个broker的filter信息.
			private final HashMap<String/* brokerAddr */, List<String>/* Filter Server */> filterServerTable; 
			private final HashMap<String/* Namespace */, HashMap<String/* Key */, String/* Value */>> configTable
	 		//根据namespace配置区分的config表
	 	broker 维护的是所有消费者和

 供销比

工程 broker 
依赖项目的  common(通用工具) store (存储) remoting   client(与客户端的链接) srvutil(服务工具)  filter
入口主类 BrokerStartup
-->start(createBrokerController(args));
	--> createBrokerController(args)
		--> BrokerConfig brokerConfig = new BrokerConfig();
		--> NettyServerConfig nettyServerConfig = new NettyServerConfig();
		--> NettyClientConfig nettyClientConfig = new NettyClientConfig();
		--> MessageStoreConfig messageStoreConfig = new MessageStoreConfig();

	--> controller.start();
