
//mysql 协议
https://blog.csdn.net/wind520/article/details/43964821
serverSocket
https://blog.csdn.net/robinjwong/article/details/41792623
mvcc 的实现
https://liuzhengyang.github.io/2017/04/18/innodb-mvcc/
cobar服务端 源码分析
schame rute 等信息的配置
CommandCount 统计信息
[ 每个datasource的链接最大值是128 且超过此值时有警告 ]
[ 分多个库得结果处理是执行完所有库的返回结果统一写到 blockingsession/nonblockingsession , 通过信号量来控制]
[ com.alibaba.cobar.mysql.bio.executor.MultiNodeExecutor#handleSuccessOK  decrementCountAndIsZero() == true 表示处理完所有节点的返回  然后向前段写回数据]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
服务端入口 CobarStartup
	CobarServer server = CobarServer.getInstance();
	--> CobarServer INSTANCE = new CobarServer();
		--> this.config = new CobarConfig();
			--> ConfigInitializer confInit = new ConfigInitializer(); 实例化配置文件
----------------------------------------------------实例化schema信息----------------------------------------------------			
				--> SchemaLoader schemaLoader = new XMLSchemaLoader(); 实例化配置文件
				--> XMLConfigLoader configLoader = new XMLConfigLoader(schemaLoader);
					--> XMLServerLoader serverLoader = new XMLServerLoader();
						--> XMLServerLoader.load() // server.xml 文件中内容
							--> loadSystem(root); // 实例化系统的参数等信息
							--> loadUsers(root); //实例化用户 密码 以及 访问的 schemas 
							--> XMLServerLoader.cluster = new ClusterConfig(root, system.getServerPort()); 实例化集群配置 
								--> ClusterConfig.nodes = Map<String, CobarNodeConfig> loadNode(Element root,system.getServerPort()) //实例化nodes的信息
								--> ClusterConfig.groups = Map<String, List<String>> loadGroup(Element root,ClusterConfig.nodes) //实力化group的信息
----------------------------------------------------实例化routerule信息----------------------------------------------------
				--> RouteRuleInitializer.initRouteRule(schemaLoader); 实例化处理RouteRule
				--> ConfigInitializer.dataNodes = initDataNodes(configLoader);
					--> ConfigInitializer.dataNodes [Map<String, MySQLDataNode>]  <-- ClusterConfig.nodes
						--> dataNodes = new HashMap<String, MySQLDataNode>(nodeConfs.size()) 
							--> dataNode = new MySQLDataNode(dnc);	
							--> dsList = new MySQLDataSource[dsNames.length];
							--> dataNode.setSources(dsList); // 设置每个节点的数据源
						--> dataNodes.put(dataNode.getName(), dataNode); // 将
				--> this.cluster = initCobarCluster(configLoader);
					--> ConfigInitializer.cluster = new CobarCluster(configLoader.getClusterConfig())
						--> CobarCluster.nodes = new CobarNode(conf);
							--> new CobarHeartbeat(this) // 集群中每个节点的心跳的信息
								--> this.factory = new CobarDetectorFactory(); // 网络心跳Detector
									--> CobarDetectorFactory.make 创建网络连接
										--> SocketChannel channel = openSocketChannel(); // 打开网络channel
										--> new CobarDetector(channel); //
									--> CobarDetector.heartbeat() //发送心跳
								--> this.recorder = new HeartbeatRecorder(); 心跳记录信息 做统计使用的
reactor 模型启动
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
			server.startup(); //启动服务
			--> new NIOProcessor[system.getProcessors()]; 实例化NIOProcessor 线程池 启动processor 同时也是 实例化NIOReactor
			--> processors[i].startup(); 启动处理线程
			--> timer.schedule(processorCheck(), 0L, system.getProcessorCheckPeriod()); 定时执行该方法，回收部分资源
启动后端的connector---------------------------------------------------------------------------------------------------------------------------------  
			--> NIOConnector connector = new NIOConnector(NAME + "Connector");  启动connector 
			--> connector.setProcessors(processors); // 设置实例化的处理器 [ selectionKey.attach(theObject); selectionKey 附加到某个键上]
    		--> connector.start(); //启动reactor 模型 
启动zookeeper对后端节点的监听------------------------------------------------------------------------------------------------------------------------ 
    		--> zookeeperManager = ZookeeperManager.getInstance(); 实例化zk client /cobar/{clusterName}/datasource 节点
    		--> listenAndDoSwitch(this.datasourceConf); 监听该节点下childEvent
    		--> for (MySQLDataNode node : dataNodes.values()) node.init(1, 0); //实例化每个节点
    			--> this.initZookeeperData() //初始化分布式锁等ZK协调元数据  xxxxx/datasource/dataNode.name  xxxxx/lock/dataNode.name 
    		--> timer.schedule(dataNodeIdleCheck() x,x)数据节点定时连接空闲超时检查任务
    		--> timer.schedule(dataNodeHeartbeat() x,x)数据节点定时心跳任务
启动前端front的链接的管理---------------------------------------------------------------------------------------------------------------------------------  
    		--> ManagerConnectionFactory mf = new ManagerConnectionFactory(); 实例化manager
    		--> ServerConnectionFactory sf = new ServerConnectionFactory(); 实例化ServerConnection
    		--> server = new NIOAcceptor(NAME + "Server", system.getServerPort(), sf);
    		--> server.start();
    		--> timer.schedule(clusterHeartbeat(),x,x) 
    		--> doDoubleCheckHeartBeat(); 检查delay的节点的心跳
    		--> scheduledExecutorService.scheduleAtFixedRate(new PerformanceDataSender(),x,x) 将收集到的信息发送到前端即 cobar-manager
		


调用链
	 NIOAcceptor.run() 
	 	-> accept(); 获取 SelectionKey
	 		-> SocketChannel channel = serverChannel.accept(); 在selectkeys 中获取sc
	 		-> FrontendConnection c = factory.make(channel);   实例化FC	
	 		-> NIOProcessor processor = nextProcessor();	   Round-Robin 方式选则Processor
	 		-> processor.postRegister(c);					   注册到Reactor 中去
	 			-> 到NIOReactor 注册读事件
	 				-> AbstractConnection.read
	 					-> FrontendConnection.handle
	 						-> FrontendAuthenticator.handle  	前段认证 校验用户等的信息
	 						-> success(auth);					通过认证, 设置后端
	 							-> source.setHandler(new FrontendCommandHandler(source));  注册回调  重新设置handler 为第二次读做准备, 处理MySQL 返回的读
	 							-> source.write(source.writeToBuffer(AUTH_OK, buffer));    写入->
	 								-> NIOReactor.W.write
	 								-> AbstractConnection.writeByEvent 唤醒写事件
	 								-> NIOReactor.R.write
	FrontendCommandHandler.handle
		-> @Override
	    public void handle(byte[] data) {
	        switch (data[4]) {
	        case MySQLPacket.COM_INIT_DB:
	            commands.doInitDB();
	            source.initDB(data);
	            break;
	        case MySQLPacket.COM_QUERY:
	            commands.doQuery();
	            source.query(data);
	            break;
	        case MySQLPacket.COM_PING:
	            commands.doPing();
	            source.ping();
	            break;
	        case MySQLPacket.COM_QUIT:
	            commands.doQuit();
	            source.close();
	            break;
	        case MySQLPacket.COM_PROCESS_KILL:
	            commands.doKill();
	            source.kill(data);
	            break;
	        case MySQLPacket.COM_STMT_PREPARE:
	            commands.doStmtPrepare();
	            source.stmtPrepare(data);
	            break;
	        case MySQLPacket.COM_STMT_EXECUTE:
	            commands.doStmtExecute();
	            source.stmtExecute(data);
	            break;
	        case MySQLPacket.COM_STMT_CLOSE:
	            commands.doStmtClose();
	            source.stmtClose(data);
	            break;
	        case MySQLPacket.COM_HEARTBEAT:
	            commands.doHeartbeat();
	            source.heartbeat(data);
	            break;
	        default:
	            commands.doOther();
	            source.writeErrMessage(ErrorCode.ER_UNKNOWN_COM_ERROR, "Unknown command");
	    ->  FrontendConnection.query(sql)
	    	-> ServerQueryHandler.query(sql)
	    		-> SelectHandler.handle(stmt,ServerConnection,offs)
	    			-> ServerConnection.execute(sql, type)
	    				-> SchemaConfig schema = CobarServer.getInstance().getConfig().getSchemas().get(db);
	    				-> RouteResultset rrs = ServerRouter.route(schema, sql, this.charset, this); // 拆分sql 设置结果集  设置节点等的信息
	    					-> RouteResultset rrs = new RouteResultset(stmt);//route ResultSet
	    					// 检查是否含有cobar hint
        					-> int prefixIndex = HintRouter.indexOfPrefix(stmt);
	    				-> BlockingSession.execute(rrs, type); // blockingsession是指mysql的connection 是blocking 的  然后返回处理的结果
	    					-> SingleNodeExecutor.execute
	    					-> MultiNodeExecutor.execute
	    						-> MultiNodeExecutor.newExecute
	    						-> CobarConfig conf = CobarServer.getInstance().getConfig();  //找到后端的数据节点
	    						-> final MySQLDataNode dn = conf.getDataNodes().get(rrn.getName()); //找到MySQLDataNode
	    						-> c = (i == DEFAULT_REPLICA_INDEX) ? dn.getChannel() : dn.getChannel(i); // c 依赖于
	    						  // 其中依赖关系 : Channel --> MySQLDataNode.getChannel --> MySQLDataSource.getChannel
	    						   --> Channel c = factory.make(this); --> c.connect(node.getConfig().getWaitTimeout());
	    						   // 里面包含了和mysql的服务器的握手协议等等的信息
	    						   --> MySQLChannel.connection 链接
	    						-> (MySQLChannel) Channel old = ss.getTarget().put(rrn, c);	// 重新放入到blokingSession/nonblockingsession
	    						-> execute0(rrn, c, autocommit, ss, flag);
	    						  // 执行并等待返回
            					-> BinaryPacket bin = ((MySQLChannel) c).execute(rrn, sc, autocommit); //从后端mysql中得到对应的数据
            					// 只要有任何的失败都返回
            					-> handleSuccessOK(ss, rrn, autocommit, ok); || handleFailure(ss, rrn, new BinaryErrInfo((MySQLChannel) c, bin, sc, rrn));  //处理正确或者失败的结果
            						-> icExecutor.commit(ok, ss, ss.getTarget().size());	 // 前端非事务模式，后端事务模式，则需要自动递交后端事务。
            						-> ok.write(source);  //向nioreactor 注册写事件  将sql 执行结果返回给前端
------------------------------------------------------------------------------------------------------------
// mysqlChannel  连接数据库的
	socket = new Socket();
    socket.setTcpNoDelay(true);
    socket.setTrafficClass(0x04 | 0x10);
    socket.setPerformancePreferences(0, 2, 1);
    socket.setReceiveBufferSize(RECV_BUFFER_SIZE);
    socket.setSendBufferSize(SEND_BUFFER_SIZE);
    socket.connect(new InetSocketAddress(dsc.getHost(), dsc.getPort()), SOCKET_CONNECT_TIMEOUT);
    in = new BufferedInputStream(socket.getInputStream(), INPUT_STREAM_BUFFER);
    out = new BufferedOutputStream(socket.getOutputStream(), OUTPUT_STREAM_BUFFER);
------------------------------------------------------------------------------------------------------------
// 这个是关联mysqlchannel 的关键一环
RouteResultsetNode  根据sql解析出来的
    private final String name; // 数据节点名称
    private final int replicaIndex;// 数据源编号
    private final String statement; // 执行的语句
------------------------------------------------------------------------------------------------------------
// 计算路由的核心代码
    public static RouteResultset route(SchemaConfig schema, String stmt, String charset, Object info)
            throws SQLNonTransientException {
        RouteResultset rrs = new RouteResultset(stmt);

        // 检查是否含有cobar hint  如果有hint 直接去对应的库里面查询
        int prefixIndex = HintRouter.indexOfPrefix(stmt);
        if (prefixIndex >= 0) {
            HintRouter.routeFromHint(info, schema, rrs, prefixIndex, stmt);
            return rrs;
        }

        // 检查schema是否含有拆分库
        if (schema.isNoSharding()) {
            if (schema.isKeepSqlSchema()) {
                SQLStatement ast = SQLParserDelegate.parse(stmt, charset == null
                        ? MySQLParser.DEFAULT_CHARSET : charset);
                PartitionKeyVisitor visitor = new PartitionKeyVisitor(schema.getTables());   //获取的表的分区key
                visitor.setTrimSchema(schema.getName());
                ast.accept(visitor);
                if (visitor.isSchemaTrimmed()) {
                    stmt = genSQL(ast, stmt);
                }
            }
            RouteResultsetNode[] nodes = new RouteResultsetNode[1];
            nodes[0] = new RouteResultsetNode(schema.getDataNode(), stmt);
            rrs.setNodes(nodes);
            return rrs;
        }

        // 生成和展开AST   根据传过来的statement 计算出
        SQLStatement ast = SQLParserDelegate.parse(stmt, charset == null ? MySQLParser.DEFAULT_CHARSET : charset);
        PartitionKeyVisitor visitor = new PartitionKeyVisitor(schema.getTables());
        visitor.setTrimSchema(schema.isKeepSqlSchema() ? schema.getName() : null);
        ast.accept(visitor);

        // 如果sql包含用户自定义的schema，则路由到default节点
        if (schema.isKeepSqlSchema() && visitor.isCustomedSchema()) {
            if (visitor.isSchemaTrimmed()) {
                stmt = genSQL(ast, stmt);
            }
            RouteResultsetNode[] nodes = new RouteResultsetNode[1];
            nodes[0] = new RouteResultsetNode(schema.getDataNode(), stmt);
            rrs.setNodes(nodes);
            return rrs;
        }

        // 元数据语句路由
        if (visitor.isTableMetaRead()) {
            MetaRouter.routeForTableMeta(rrs, schema, ast, visitor, stmt);
            if (visitor.isNeedRewriteField()) {
                rrs.setFlag(RouteResultset.REWRITE_FIELD);
            }
            return rrs;
        }

        // 匹配规则
        TableConfig matchedTable = null;
        RuleConfig rule = null;
        Map<String, List<Object>> columnValues = null;
        Map<String, Map<String, List<Object>>> astExt = visitor.getColumnValue();
        Map<String, TableConfig> tables = schema.getTables();
        // key 表名  value Map<Column, List<值>>  Map<String, List<Object>>
        ft: for (Entry<String, Map<String, List<Object>>> e : astExt.entrySet()) {
            Map<String, List<Object>> col2Val = e.getValue();
            TableConfig tc = tables.get(e.getKey());
            if (tc == null) {
                continue;
            }
            if (matchedTable == null) {
                matchedTable = tc;
            }
            if (col2Val == null || col2Val.isEmpty()) {
                continue;
            }
            TableRuleConfig tr = tc.getRule();
            if (tr != null) {
                for (RuleConfig rc : tr.getRules()) {
                    boolean match = true;
                    // 必须匹配规则中的每个字段
                    for (String ruleColumn : rc.getColumns()) {
                        match &= col2Val.containsKey(ruleColumn);
                    }
                    if (match) {
                        columnValues = col2Val;
                        rule = rc;
                        matchedTable = tc;
                        break ft;
                    }
                }
            }
        }
        // 表级别只会有单个库
        // 规则匹配处理，表级别和列级别。
        if (matchedTable == null) {
            String sql = visitor.isSchemaTrimmed() ? genSQL(ast, stmt) : stmt;
            RouteResultsetNode[] rn = new RouteResultsetNode[1];
            if ("".equals(schema.getDataNode()) && isSystemReadSQL(ast)) {
                rn[0] = new RouteResultsetNode(schema.getRandomDataNode(), sql);
            } else {
                rn[0] = new RouteResultsetNode(schema.getDataNode(), sql);
            }
            rrs.setNodes(rn);
            return rrs;
        }
        if (rule == null) {
            if (matchedTable.isRuleRequired()) {
                throw new IllegalArgumentException("route rule for table " + matchedTable.getName() + " is required: "
                        + stmt);
            }
            String[] dataNodes = matchedTable.getDataNodes();
            String sql = visitor.isSchemaTrimmed() ? genSQL(ast, stmt) : stmt;
            RouteResultsetNode[] rn = new RouteResultsetNode[dataNodes.length];
            for (int i = 0; i < dataNodes.length; ++i) {
                rn[i] = new RouteResultsetNode(dataNodes[i], sql);
            }
            rrs.setNodes(rn);
            setGroupFlagAndLimit(rrs, visitor);
            return rrs;
        }

        // 规则计算
        validateAST(ast, matchedTable, rule, visitor);
        Map<Integer, List<Object[]>> dnMap = ruleCalculate(matchedTable, rule, columnValues);
        if (dnMap == null || dnMap.isEmpty()) {
            throw new IllegalArgumentException("No target dataNode for rule " + rule);
        }

        // 判断路由结果是单库还是多库
        if (dnMap.size() == 1) {
            String dataNode = matchedTable.getDataNodes()[dnMap.keySet().iterator().next()];
            String sql = visitor.isSchemaTrimmed() ? genSQL(ast, stmt) : stmt;
            RouteResultsetNode[] rn = new RouteResultsetNode[1];
            rn[0] = new RouteResultsetNode(dataNode, sql);
            rrs.setNodes(rn);
        } else {
            RouteResultsetNode[] rn = new RouteResultsetNode[dnMap.size()];
            if (ast instanceof DMLInsertReplaceStatement) {
                DMLInsertReplaceStatement ir = (DMLInsertReplaceStatement) ast;
                dispatchInsertReplace(rn, ir, rule.getColumns(), dnMap, matchedTable, stmt, visitor);
            } else {
                dispatchWhereBasedStmt(rn, ast, rule.getColumns(), dnMap, matchedTable, stmt, visitor);
            }
            rrs.setNodes(rn);
            setGroupFlagAndLimit(rrs, visitor);
        }

        return rrs;
    }
